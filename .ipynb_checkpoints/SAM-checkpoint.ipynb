{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5y0No-rYymWc"
   },
   "source": [
    "# SAM Notebook for Mask Generation Based Off of a Bounding Box\n",
    "About: Created to generate a segmentation mask using Segment Anything (https://segment-anything.com/) prompted by bounding boxe\n",
    "<br> This notebook:\n",
    "\n",
    "\n",
    "1.   Creates bounding boxes (one per img) for a list of input images interactively using OpenCV\n",
    "2.   Runs Segmentation for the bounding box\n",
    "\n",
    "<br> This is to be used as a preprocessing step for object reconstruction using nvdiffrc where a binary mask is required in the alpha channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "theyLxClpVv2"
   },
   "source": [
    "## Environment Set Up\n",
    "For colab notebook - run to install required libraries,\n",
    "Set runtime to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_Pc6yjvTsLAO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LeuJsp-wpJ2_",
    "outputId": "d68f3f42-47b4-4ac2-d1db-77e1ee74071b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' print(\"PyTorch version:\", torch.__version__)\\nprint(\"Torchvision version:\", torchvision.__version__)\\n!{sys.executable} -m pip install opencv-python matplotlib\\n!{sys.executable} -m pip install \\'git+https://github.com/facebookresearch/segment-anything.git\\'\\n\\n!mkdir images\\n!wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\\n\\n!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "!{sys.executable} -m pip install opencv-python matplotlib\n",
    "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "\n",
    "!mkdir images\n",
    "!wget -P images https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/images/dog.jpg\n",
    "\n",
    "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tu2FUgI4pTz4"
   },
   "source": [
    "### Load Images and SAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6UTwuo8Fr9qm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Set up SAM\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"  \n",
    "model_type = \"vit_h\"\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yEkXzHywukVR"
   },
   "outputs": [],
   "source": [
    "# Input Images\n",
    "from matplotlib.widgets import RectangleSelector\n",
    "from IPython.display import clear_output\n",
    "\n",
    "image_folder = \"output_frames\"  # Upload images here from frame extraction script\n",
    "box_save_path = \"bboxes.npy\"  # output file for bboxes\n",
    "mask_folder = \"masks/\"\n",
    "os.makedirs(mask_folder, exist_ok=True)\n",
    "\n",
    "image_files = sorted([\n",
    "    f for f in os.listdir(image_folder) if f.endswith((\".jpg\", \".png\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_nSCwUBvX0D"
   },
   "source": [
    "## Interactve Bounding Box Drawing\n",
    "This function will allow drawing a bounding box with a mouse using open cv for the object that will be automatically segmented - alternatively can use clicking functionality like in META demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "g__2gojnvsEa"
   },
   "outputs": [],
   "source": [
    "def get_bbox(image, window_name=\"Draw Bounding Box\"):\n",
    "    \"\"\"\n",
    "    Draw a single bounding box on an image.\n",
    "    Controls:\n",
    "    - ENTER/SPACE = confirm box\n",
    "    - ESC         = skip image\n",
    "    - BACKSPACE   = go back to previous image\n",
    "    - Q           = quit the entire loop\n",
    "    \"\"\"\n",
    "    PADDING = 20\n",
    "    image_padded = cv2.copyMakeBorder(\n",
    "        image,\n",
    "        top=PADDING,\n",
    "        bottom=PADDING,\n",
    "        left=PADDING,\n",
    "        right=PADDING,\n",
    "        borderType=cv2.BORDER_CONSTANT,\n",
    "        value=[255, 255, 255]  # white\n",
    "    )\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    clone = image_padded.copy()\n",
    "    box = []\n",
    "    dragging = False\n",
    "    cursor = (0, 0)\n",
    "    \n",
    "    def clamp_point(pt):\n",
    "        x, y = pt\n",
    "        # Clamp inside padded image bounds\n",
    "        x = max(0, min(x, width + 2 * PADDING))\n",
    "        y = max(0, min(y, height + 2 * PADDING))\n",
    "        return (x, y)\n",
    "        \n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        nonlocal box, dragging, cursor\n",
    "        cursor = (x, y)\n",
    "\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            box = [(x, y)]\n",
    "            dragging = True\n",
    "\n",
    "        elif event == cv2.EVENT_MOUSEMOVE and dragging:\n",
    "            box = [box[0], (x, y)]\n",
    "\n",
    "        elif event == cv2.EVENT_LBUTTONUP:\n",
    "            box.append((x, y))\n",
    "            dragging = False\n",
    "\n",
    "    cv2.namedWindow(window_name)\n",
    "    cv2.setMouseCallback(window_name, mouse_callback)\n",
    "\n",
    "    while True:\n",
    "        frame = clone.copy()\n",
    "\n",
    "        # Draw crosshair\n",
    "        cx, cy = cursor\n",
    "        cv2.line(frame, (cx, 0), (cx, frame.shape[0]), (0, 255, 0), 1)\n",
    "        cv2.line(frame, (0, cy), (frame.shape[1], cy), (0, 255, 0), 1)\n",
    "\n",
    "        # Draw bounding box\n",
    "        if len(box) >= 2:\n",
    "            x1, y1 = box[0]\n",
    "            x2, y2 = box[1]\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "        # Overlay instructions\n",
    "        cv2.putText(frame, \"ENTER/SPACE=confirm | ESC=skip | BACKSPACE=go back | Q=quit\",\n",
    "                    (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow(window_name, frame)\n",
    "        key = cv2.waitKey(10)\n",
    "\n",
    "        if key in [13, 32]:  # ENTER or SPACE\n",
    "            if len(box) >= 2:\n",
    "                x1, y1 = box[0]\n",
    "                x2, y2 = box[1]\n",
    "\n",
    "                # Clamp box coordinates back inside original image bounds\n",
    "                x_min = max(0, min(x1, x2) - PADDING)\n",
    "                y_min = max(0, min(y1, y2) - PADDING)\n",
    "                x_max = min(width, max(x1, x2) - PADDING)\n",
    "                y_max = min(height, max(y1, y2) - PADDING)\n",
    "                \n",
    "                cv2.destroyAllWindows()\n",
    "                return [x_min, y_min, x_max, y_max]\n",
    "\n",
    "        elif key == 27:  # ESC\n",
    "            cv2.destroyAllWindows()\n",
    "            return None\n",
    "\n",
    "        elif key == ord('q'):  # Quit all\n",
    "            cv2.destroyAllWindows()\n",
    "            return \"quit\"\n",
    "\n",
    "        elif key in [8, 127]:  # BACKSPACE \n",
    "            cv2.destroyAllWindows()\n",
    "            return \"back\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGiF703Yvy09"
   },
   "source": [
    "Iterate through images and save - draw box when the window opens up then press a key to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8cBM_zAZw2SY",
    "outputId": "ca0c8ebc-8098-47b8-b9fd-32f1dbb30b07"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 14:28:50.209 Python[57045:49760088] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-06-09 14:28:50.209 Python[57045:49760088] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved box for frame_0000.jpg\n",
      "Saved box for frame_0001.jpg\n",
      "Saved box for frame_0002.jpg\n",
      "Saved box for frame_0003.jpg\n",
      "Saved box for frame_0004.jpg\n",
      "Saved box for frame_0005.jpg\n",
      "Saved box for frame_0006.jpg\n",
      "Saved box for frame_0007.jpg\n",
      "Saved box for frame_0008.jpg\n",
      "Going back to frame_0008.jpg\n",
      "Saved box for frame_0008.jpg\n",
      "Saved box for frame_0009.jpg\n",
      "Saved box for frame_0010.jpg\n",
      "Saved box for frame_0011.jpg\n",
      "Saved box for frame_0012.jpg\n",
      "Saved box for frame_0013.jpg\n",
      "Going back to frame_0013.jpg\n",
      "Saved box for frame_0013.jpg\n",
      "Skipped frame_0014.jpg\n",
      "Stopped by user.\n"
     ]
    }
   ],
   "source": [
    "bboxes = {}\n",
    "history = []\n",
    "\n",
    "i = 0\n",
    "while i < len(image_files):\n",
    "    fname = image_files[i]\n",
    "    img_path = os.path.join(image_folder, fname)\n",
    "    image = cv2.imread(img_path)\n",
    "\n",
    "    if image is None:\n",
    "        print(f\"Could not load {fname}\")\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    result = get_bbox(image, window_name=f\"Draw box for {fname}\")\n",
    "\n",
    "    if result == \"quit\":\n",
    "        print(\"Stopped by user.\")\n",
    "        break\n",
    "\n",
    "    elif result == \"back\":\n",
    "        if history:\n",
    "            last_fname = history.pop()\n",
    "            bboxes.pop(last_fname, None)\n",
    "            i -= 1  # go back\n",
    "            print(f\"Going back to {image_files[i]}\")\n",
    "        else:\n",
    "            print(\"Nothing to go back to.\")\n",
    "        continue\n",
    "\n",
    "    elif result is None:\n",
    "        print(f\"Skipped {fname}\")\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        bboxes[fname] = result\n",
    "        history.append(fname)\n",
    "        np.save(box_save_path, bboxes) #save\n",
    "        print(f\"Saved box for {fname}\")\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2ksLSX-xVEZ"
   },
   "source": [
    "## Run SAM Map Prediction Based off of Bounding Boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XpsZh2d4yWk0"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m image_bgr = cv2.imread(img_path)\n\u001b[32m      4\u001b[39m image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m input_box = np.array(box)\n\u001b[32m      8\u001b[39m masks, scores, logits = predictor.predict(\n\u001b[32m      9\u001b[39m     box=input_box[\u001b[38;5;28;01mNone\u001b[39;00m, :],\n\u001b[32m     10\u001b[39m     multimask_output=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     11\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/mesh_reconstruction/.venv/lib/python3.13/site-packages/segment_anything/predictor.py:60\u001b[39m, in \u001b[36mSamPredictor.set_image\u001b[39m\u001b[34m(self, image, image_format)\u001b[39m\n\u001b[32m     57\u001b[39m input_image_torch = torch.as_tensor(input_image, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     58\u001b[39m input_image_torch = input_image_torch.permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m).contiguous()[\u001b[38;5;28;01mNone\u001b[39;00m, :, :, :]\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_torch_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/mesh_reconstruction/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/mesh_reconstruction/.venv/lib/python3.13/site-packages/segment_anything/predictor.py:89\u001b[39m, in \u001b[36mSamPredictor.set_torch_image\u001b[39m\u001b[34m(self, transformed_image, original_image_size)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m.input_size = \u001b[38;5;28mtuple\u001b[39m(transformed_image.shape[-\u001b[32m2\u001b[39m:])\n\u001b[32m     88\u001b[39m input_image = \u001b[38;5;28mself\u001b[39m.model.preprocess(transformed_image)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28mself\u001b[39m.features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28mself\u001b[39m.is_image_set = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/mesh_reconstruction/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/mesh_reconstruction/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/mesh_reconstruction/.venv/lib/python3.13/site-packages/segment_anything/modeling/image_encoder.py:112\u001b[39m, in \u001b[36mImageEncoderViT.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    109\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.pos_embed\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     x = \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m x = \u001b[38;5;28mself\u001b[39m.neck(x.permute(\u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m))\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/mesh_reconstruction/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/mesh_reconstruction/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/mesh_reconstruction/.venv/lib/python3.13/site-packages/segment_anything/modeling/image_encoder.py:174\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    171\u001b[39m     H, W = x.shape[\u001b[32m1\u001b[39m], x.shape[\u001b[32m2\u001b[39m]\n\u001b[32m    172\u001b[39m     x, pad_hw = window_partition(x, \u001b[38;5;28mself\u001b[39m.window_size)\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.window_size > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/mesh_reconstruction/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/mesh_reconstruction/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/mesh_reconstruction/.venv/lib/python3.13/site-packages/segment_anything/modeling/image_encoder.py:234\u001b[39m, in \u001b[36mAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    231\u001b[39m attn = (q * \u001b[38;5;28mself\u001b[39m.scale) @ k.transpose(-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_rel_pos:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     attn = \u001b[43madd_decomposed_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m attn = attn.softmax(dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    237\u001b[39m x = (attn @ v).view(B, \u001b[38;5;28mself\u001b[39m.num_heads, H, W, -\u001b[32m1\u001b[39m).permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m).reshape(B, H, W, -\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/mesh_reconstruction/.venv/lib/python3.13/site-packages/segment_anything/modeling/image_encoder.py:350\u001b[39m, in \u001b[36madd_decomposed_rel_pos\u001b[39m\u001b[34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[39m\n\u001b[32m    348\u001b[39m k_h, k_w = k_size\n\u001b[32m    349\u001b[39m Rh = get_rel_pos(q_h, k_h, rel_pos_h)\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m Rw = \u001b[43mget_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    352\u001b[39m B, _, dim = q.shape\n\u001b[32m    353\u001b[39m r_q = q.reshape(B, q_h, q_w, dim)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/mesh_reconstruction/.venv/lib/python3.13/site-packages/segment_anything/modeling/image_encoder.py:292\u001b[39m, in \u001b[36mget_rel_pos\u001b[39m\u001b[34m(q_size, k_size, rel_pos)\u001b[39m\n\u001b[32m    288\u001b[39m         x = x[:, :H, :W, :].contiguous()\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_rel_pos\u001b[39m(q_size: \u001b[38;5;28mint\u001b[39m, k_size: \u001b[38;5;28mint\u001b[39m, rel_pos: torch.Tensor) -> torch.Tensor:\n\u001b[32m    293\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[33;03m    Get relative positional embeddings according to the relative positions of\u001b[39;00m\n\u001b[32m    295\u001b[39m \u001b[33;03m        query and key sizes.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    302\u001b[39m \u001b[33;03m        Extracted positional embeddings according to relative positions.\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    304\u001b[39m     max_rel_dist = \u001b[38;5;28mint\u001b[39m(\u001b[32m2\u001b[39m * \u001b[38;5;28mmax\u001b[39m(q_size, k_size) - \u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for fname, box in bboxes.items():\n",
    "    img_path = os.path.join(image_folder, fname)\n",
    "    image_bgr = cv2.imread(img_path)\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    predictor.set_image(image_rgb)\n",
    "    input_box = np.array(box)\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        box=input_box[None, :],\n",
    "        multimask_output=False\n",
    "    )\n",
    "\n",
    "    mask = masks[0]\n",
    "    save_path = os.path.join(mask_folder, fname.replace(\".jpg\", \".png\"))\n",
    "    cv2.imwrite(save_path, (mask * 255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpLBHQgWyfdC"
   },
   "source": [
    "Display Mask for Debbugging"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
